{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_SRGAN_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVe6VSuHnvb4",
        "outputId": "dc7b6dd3-3372-48ff-f806-e55b678c21c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.data.experimental import AUTOTUNE\n",
        "\n",
        "\n",
        "class my_dataset:\n",
        "    def __init__(self,\n",
        "                 scale=2,\n",
        "                 subset='train',\n",
        "                 downgrade='bicubic',\n",
        "                 images_dir='/content/drive/MyDrive/image_dir/div2k_images',\n",
        "                 caches_dir='/content/drive/MyDrive/image_dir/div2k_caches'):\n",
        "\n",
        "        self._ntire_2018 = True\n",
        "\n",
        "        _scales = [2, 3, 4, 8]\n",
        "\n",
        "        if scale in _scales:\n",
        "            self.scale = scale\n",
        "        else:\n",
        "            raise ValueError(f'scale must be in ${_scales}')\n",
        "\n",
        "        if subset == 'train':\n",
        "            self.image_ids = range(1, 1088)\n",
        "        elif subset == 'valid':\n",
        "            self.image_ids = range(801, 901)\n",
        "        else:\n",
        "            raise ValueError(\"subset must be 'train' or 'valid'\")\n",
        "\n",
        "        _downgrades_a = ['bicubic', 'unknown']\n",
        "        _downgrades_b = ['mild', 'difficult']\n",
        "\n",
        "        if scale == 8 and downgrade != 'bicubic':\n",
        "            raise ValueError(f'scale 8 only allowed for bicubic downgrade')\n",
        "\n",
        "        if downgrade in _downgrades_b and scale != 4:\n",
        "            raise ValueError(f'{downgrade} downgrade requires scale 4')\n",
        "\n",
        "        if downgrade == 'bicubic' and scale == 8:\n",
        "            self.downgrade = 'x8'\n",
        "        elif downgrade in _downgrades_b:\n",
        "            self.downgrade = downgrade\n",
        "        else:\n",
        "            self.downgrade = downgrade\n",
        "            self._ntire_2018 = False\n",
        "\n",
        "        self.subset = subset\n",
        "        self.images_dir = images_dir\n",
        "        self.caches_dir = caches_dir\n",
        "\n",
        "        os.makedirs(images_dir, exist_ok=True)\n",
        "        os.makedirs(caches_dir, exist_ok=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def dataset(self, batch_size=16, repeat_count=None, random_transform=True):\n",
        "        ds = tf.data.Dataset.zip((self.lr_dataset(), self.hr_dataset()))\n",
        "        if random_transform:\n",
        "            ds = ds.map(lambda lr, hr: random_crop(lr, hr, scale=self.scale), num_parallel_calls=AUTOTUNE)\n",
        "            ds = ds.map(random_rotate, num_parallel_calls=AUTOTUNE)\n",
        "            ds = ds.map(random_flip, num_parallel_calls=AUTOTUNE)\n",
        "        ds = ds.batch(batch_size)\n",
        "        ds = ds.repeat(repeat_count)\n",
        "        ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "        return ds\n",
        "\n",
        "    def hr_dataset(self):\n",
        "\n",
        "        ds = self._images_dataset(self._hr_image_files()).cache(self._hr_cache_file())\n",
        "\n",
        "        if not os.path.exists(self._hr_cache_index()):\n",
        "            self._populate_cache(ds, self._hr_cache_file())\n",
        "\n",
        "        return ds\n",
        "\n",
        "    def lr_dataset(self):\n",
        "\n",
        "        ds = self._images_dataset(self._lr_image_files()).cache(self._lr_cache_file())\n",
        "\n",
        "        if not os.path.exists(self._lr_cache_index()):\n",
        "            self._populate_cache(ds, self._lr_cache_file())\n",
        "\n",
        "        return ds\n",
        "\n",
        "    def _hr_cache_file(self):\n",
        "        return os.path.join(self.caches_dir, f'DIV2K_{self.subset}_HR.cache')\n",
        "\n",
        "    def _lr_cache_file(self):\n",
        "        return os.path.join(self.caches_dir, f'DIV2K_{self.subset}_LR_{self.downgrade}_X{self.scale}.cache')\n",
        "\n",
        "    def _hr_cache_index(self):\n",
        "        return f'{self._hr_cache_file()}.index'\n",
        "\n",
        "    def _lr_cache_index(self):\n",
        "        return f'{self._lr_cache_file()}.index'\n",
        "\n",
        "    def _hr_image_files(self):\n",
        "        images_dir = self._hr_images_dir()\n",
        "        return [os.path.join(images_dir, f'{image_id:04}.png') for image_id in self.image_ids]\n",
        "\n",
        "    def _lr_image_files(self):\n",
        "        images_dir = self._lr_images_dir()\n",
        "        return [os.path.join(images_dir, self._lr_image_file(image_id)) for image_id in self.image_ids]\n",
        "\n",
        "    def _lr_image_file(self, image_id):\n",
        "        if not self._ntire_2018 or self.scale == 8:\n",
        "            return f'{image_id:04}x{self.scale}.png'\n",
        "        else:\n",
        "            return f'{image_id:04}x{self.scale}{self.downgrade[0]}.png'\n",
        "\n",
        "    def _hr_images_dir(self):\n",
        "        return os.path.join(self.images_dir, f'DIV2K_{self.subset}_HR')\n",
        "\n",
        "    def _lr_images_dir(self):\n",
        "        if self._ntire_2018:\n",
        "            return os.path.join(self.images_dir, f'DIV2K_{self.subset}_LR_{self.downgrade}')\n",
        "        else:\n",
        "            return os.path.join(self.images_dir, f'DIV2K_{self.subset}_LR_{self.downgrade}', f'X{self.scale}')\n",
        "\n",
        "    def _hr_images_archive(self):\n",
        "        return f'DIV2K_{self.subset}_HR.zip'\n",
        "\n",
        "    def _lr_images_archive(self):\n",
        "        if self._ntire_2018:\n",
        "            return f'DIV2K_{self.subset}_LR_{self.downgrade}.zip'\n",
        "        else:\n",
        "            return f'DIV2K_{self.subset}_LR_{self.downgrade}_X{self.scale}.zip'\n",
        "\n",
        "    @staticmethod\n",
        "    def _images_dataset(image_files):\n",
        "        ds = tf.data.Dataset.from_tensor_slices(image_files)\n",
        "        ds = ds.map(tf.io.read_file)\n",
        "        ds = ds.map(lambda x: tf.image.decode_png(x, channels=3), num_parallel_calls=AUTOTUNE)\n",
        "        return ds\n",
        "\n",
        "    @staticmethod\n",
        "    def _populate_cache(ds, cache_file):\n",
        "        print(f'Caching decoded images in {cache_file} ...')\n",
        "        for _ in ds: pass\n",
        "        print(f'Cached decoded images in {cache_file}.')\n",
        "\n",
        "def random_crop(lr_img, hr_img, hr_crop_size=96, scale=2):\n",
        "    lr_crop_size = hr_crop_size // scale\n",
        "    lr_img_shape = tf.shape(lr_img)[:2]\n",
        "\n",
        "    lr_w = tf.random.uniform(shape=(), maxval=lr_img_shape[1] - lr_crop_size + 1, dtype=tf.int32)\n",
        "    lr_h = tf.random.uniform(shape=(), maxval=lr_img_shape[0] - lr_crop_size + 1, dtype=tf.int32)\n",
        "\n",
        "    hr_w = lr_w * scale\n",
        "    hr_h = lr_h * scale\n",
        "\n",
        "    lr_img_cropped = lr_img[lr_h:lr_h + lr_crop_size, lr_w:lr_w + lr_crop_size]\n",
        "    hr_img_cropped = hr_img[hr_h:hr_h + hr_crop_size, hr_w:hr_w + hr_crop_size]\n",
        "\n",
        "    return lr_img_cropped, hr_img_cropped\n",
        "\n",
        "\n",
        "def random_flip(lr_img, hr_img):\n",
        "    rn = tf.random.uniform(shape=(), maxval=1)\n",
        "    return tf.cond(rn < 0.5,\n",
        "                   lambda: (lr_img, hr_img),\n",
        "                   lambda: (tf.image.flip_left_right(lr_img),\n",
        "                            tf.image.flip_left_right(hr_img)))\n",
        "\n",
        "\n",
        "def random_rotate(lr_img, hr_img):\n",
        "    rn = tf.random.uniform(shape=(), maxval=4, dtype=tf.int32)\n",
        "    return tf.image.rot90(lr_img, rn), tf.image.rot90(hr_img, rn)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HsMkjX8ieXnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = my_dataset(scale=4,           \n",
        "                     downgrade='bicubic', \n",
        "                     subset='train')      "
      ],
      "metadata": {
        "id": "Qm9VtJwPh7P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   \n",
        "train_ds = train_loader.dataset(batch_size=16,         \n",
        "                                random_transform=True, \n",
        "                                repeat_count=None)     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afl09bj1h7W1",
        "outputId": "0538cd6b-63de-4dbe-af2a-4832a3e8fcfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching decoded images in /content/drive/MyDrive/image_dir/div2k_caches/DIV2K_train_LR_bicubic_X4.cache ...\n",
            "Cached decoded images in /content/drive/MyDrive/image_dir/div2k_caches/DIV2K_train_LR_bicubic_X4.cache.\n",
            "Caching decoded images in /content/drive/MyDrive/image_dir/div2k_caches/DIV2K_train_HR.cache ...\n",
            "Cached decoded images in /content/drive/MyDrive/image_dir/div2k_caches/DIV2K_train_HR.cache.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "import cv2 \n",
        "import numpy as np\n",
        "import sys"
      ],
      "metadata": {
        "id": "tGc8_Ex4L9Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def load_image(path):\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "\n",
        "def plot_sample(lr, sr):\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    images = [lr, sr]\n",
        "    titles = ['LR', f'SR (x{sr.shape[0] // lr.shape[0]})']\n",
        "\n",
        "    for i, (img, title) in enumerate(zip(images, titles)):\n",
        "        plt.subplot(1, 2, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(title)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n"
      ],
      "metadata": {
        "id": "z8S2TDnzMBpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resolve(model, lr_batch):\n",
        "    lr_batch = tf.cast(lr_batch, tf.float32)\n",
        "    sr_batch = model(lr_batch)\n",
        "    sr_batch = tf.clip_by_value(sr_batch, 0, 255)\n",
        "    sr_batch = tf.round(sr_batch)\n",
        "    sr_batch = tf.cast(sr_batch, tf.uint8)\n",
        "    return sr_batch\n",
        "\n",
        "def resolve_single(model, lr):\n",
        "    return resolve(model, tf.expand_dims(lr, axis=0))[0]\n",
        "\n",
        "def pixel_shuffle(scale):\n",
        "    return lambda x: tf.nn.depth_to_space(x, scale)\n",
        "\n",
        "def normalize_01(x):\n",
        "    return x / 255.0\n",
        "\n",
        "\n",
        "def normalize_m11(x):\n",
        "    return x / 127.5 - 1\n",
        "\n",
        "\n",
        "def denormalize_m11(x):\n",
        "    return (x + 1) * 127.5"
      ],
      "metadata": {
        "id": "LiXDaGNpMfBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Add, BatchNormalization, Conv2D, Dense, Flatten, Input, LeakyReLU, PReLU, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.vgg19 import VGG19"
      ],
      "metadata": {
        "id": "kARP_kP0MfF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LR_SIZE = 24\n",
        "HR_SIZE = 96\n",
        "\n",
        "\n",
        "def upsample(x_in, num_filters):\n",
        "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x_in)\n",
        "    x = Lambda(pixel_shuffle(scale=2))(x)\n",
        "    return PReLU(shared_axes=[1, 2])(x)\n",
        "\n",
        "\n",
        "def res_block(x_in, num_filters, momentum=0.8):\n",
        "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x_in)\n",
        "    x = BatchNormalization(momentum=momentum)(x)\n",
        "    x = PReLU(shared_axes=[1, 2])(x)\n",
        "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x)\n",
        "    x = BatchNormalization(momentum=momentum)(x)\n",
        "    x = Add()([x_in, x])\n",
        "    return x\n",
        "\n",
        "\n",
        "def sr_resnet(num_filters=64, num_res_blocks=16):\n",
        "    x_in = Input(shape=(None, None, 3))\n",
        "    x = Lambda(normalize_01)(x_in)\n",
        "\n",
        "    x = Conv2D(num_filters, kernel_size=9, padding='same')(x)\n",
        "    x = x_1 = PReLU(shared_axes=[1, 2])(x)\n",
        "\n",
        "    for _ in range(num_res_blocks):\n",
        "        x = res_block(x, num_filters)\n",
        "\n",
        "    x = Conv2D(num_filters, kernel_size=3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x_1, x])\n",
        "\n",
        "    x = upsample(x, num_filters * 4)\n",
        "    x = upsample(x, num_filters * 4)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=9, padding='same', activation='tanh')(x)\n",
        "    x = Lambda(denormalize_m11)(x)\n",
        "\n",
        "    return Model(x_in, x)\n",
        "\n",
        "\n",
        "generator = sr_resnet\n",
        "\n",
        "\n",
        "def discriminator_block(x_in, num_filters, strides=1, batchnorm=True, momentum=0.8):\n",
        "    x = Conv2D(num_filters, kernel_size=3, strides=strides, padding='same')(x_in)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization(momentum=momentum)(x)\n",
        "    return LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "\n",
        "def discriminator(num_filters=64):\n",
        "    x_in = Input(shape=(HR_SIZE, HR_SIZE, 3))\n",
        "    x = Lambda(normalize_m11)(x_in)\n",
        "\n",
        "    x = discriminator_block(x, num_filters, batchnorm=False)\n",
        "    x = discriminator_block(x, num_filters, strides=2)\n",
        "\n",
        "    x = discriminator_block(x, num_filters * 2)\n",
        "    x = discriminator_block(x, num_filters * 2, strides=2)\n",
        "\n",
        "    x = discriminator_block(x, num_filters * 4)\n",
        "    x = discriminator_block(x, num_filters * 4, strides=2)\n",
        "\n",
        "    x = discriminator_block(x, num_filters * 8)\n",
        "    x = discriminator_block(x, num_filters * 8, strides=2)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(1024)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return Model(x_in, x)\n",
        "\n",
        "\n",
        "def vgg_22():\n",
        "    return _vgg(5)\n",
        "\n",
        "\n",
        "def vgg_54():\n",
        "    return _vgg(20)\n",
        "\n",
        "\n",
        "def _vgg(output_layer):\n",
        "    vgg = VGG19(input_shape=(None, None, 3), include_top=False)\n",
        "    return Model(vgg.input, vgg.layers[output_layer].output)\n"
      ],
      "metadata": {
        "id": "AS0G5VLnMfHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "DIV2K_RGB_MEAN = np.array([0.4488, 0.4371, 0.4040]) * 255\n",
        "\n",
        "\n",
        "def resolve_single(model, lr):\n",
        "    return resolve(model, tf.expand_dims(lr, axis=0))[0]\n",
        "\n",
        "\n",
        "def resolve(model, lr_batch):\n",
        "    lr_batch = tf.cast(lr_batch, tf.float32)\n",
        "    sr_batch = model(lr_batch)\n",
        "    sr_batch = tf.clip_by_value(sr_batch, 0, 255)\n",
        "    sr_batch = tf.round(sr_batch)\n",
        "    sr_batch = tf.cast(sr_batch, tf.uint8)\n",
        "    return sr_batch\n",
        "\n",
        "\n",
        "def evaluate(model, dataset):\n",
        "    psnr_values = []\n",
        "    for lr, hr in dataset:\n",
        "        sr = resolve(model, lr)\n",
        "        psnr_value = psnr(hr, sr)[0]\n",
        "        psnr_values.append(psnr_value)\n",
        "    return tf.reduce_mean(psnr_values)\n",
        "\n",
        "def normalize(x, rgb_mean=DIV2K_RGB_MEAN):\n",
        "    return (x - rgb_mean) / 127.5\n",
        "\n",
        "\n",
        "def denormalize(x, rgb_mean=DIV2K_RGB_MEAN):\n",
        "    return x * 127.5 + rgb_mean\n",
        "\n",
        "\n",
        "def normalize_01(x):\n",
        "    return x / 255.0\n",
        "\n",
        "\n",
        "def normalize_m11(x):\n",
        "    return x / 127.5 - 1\n",
        "\n",
        "\n",
        "def denormalize_m11(x):\n",
        "    return (x + 1) * 127.5\n",
        "\n",
        "def psnr(x1, x2):\n",
        "    return tf.image.psnr(x1, x2, max_val=255)\n",
        "\n",
        "def pixel_shuffle(scale):\n",
        "    return lambda x: tf.nn.depth_to_space(x, scale)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wBSQeRnyMfK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.losses import MeanAbsoluteError\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 loss,\n",
        "                 learning_rate,\n",
        "                 checkpoint_dir='./ckpt/edsr'):\n",
        "\n",
        "        self.now = None\n",
        "        self.loss = loss\n",
        "        self.checkpoint = tf.train.Checkpoint(step=tf.Variable(0),\n",
        "                                              psnr=tf.Variable(-1.0),\n",
        "                                              optimizer=Adam(learning_rate),\n",
        "                                              model=model)\n",
        "        self.checkpoint_manager = tf.train.CheckpointManager(checkpoint=self.checkpoint,\n",
        "                                                             directory=checkpoint_dir,\n",
        "                                                             max_to_keep=3)\n",
        "\n",
        "        self.restore()\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        return self.checkpoint.model\n",
        "\n",
        "    def train(self, train_dataset, valid_dataset, steps, evaluate_every=1000, save_best_only=False):\n",
        "        loss_mean = Mean()\n",
        "\n",
        "        ckpt_mgr = self.checkpoint_manager\n",
        "        ckpt = self.checkpoint\n",
        "\n",
        "        self.now = time.perf_counter()\n",
        "\n",
        "        for lr, hr in train_dataset.take(steps - ckpt.step.numpy()):\n",
        "            ckpt.step.assign_add(1)\n",
        "            step = ckpt.step.numpy()\n",
        "\n",
        "            loss = self.train_step(lr, hr)\n",
        "            loss_mean(loss)\n",
        "\n",
        "            if step % evaluate_every == 0:\n",
        "                loss_value = loss_mean.result()\n",
        "                loss_mean.reset_states()\n",
        "\n",
        "                psnr_value = self.evaluate(valid_dataset)\n",
        "\n",
        "                duration = time.perf_counter() - self.now\n",
        "                print(f'{step}/{steps}: loss = {loss_value.numpy():.3f}, PSNR = {psnr_value.numpy():3f} ({duration:.2f}s)')\n",
        "\n",
        "                if save_best_only and psnr_value <= ckpt.psnr:\n",
        "                    self.now = time.perf_counter()\n",
        "                    continue\n",
        "\n",
        "                ckpt.psnr = psnr_value\n",
        "                ckpt_mgr.save()\n",
        "\n",
        "                self.now = time.perf_counter()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, lr, hr):\n",
        "        with tf.GradientTape() as tape:\n",
        "            lr = tf.cast(lr, tf.float32)\n",
        "            hr = tf.cast(hr, tf.float32)\n",
        "\n",
        "            sr = self.checkpoint.model(lr, training=True)\n",
        "            loss_value = self.loss(hr, sr)\n",
        "\n",
        "        gradients = tape.gradient(loss_value, self.checkpoint.model.trainable_variables)\n",
        "        self.checkpoint.optimizer.apply_gradients(zip(gradients, self.checkpoint.model.trainable_variables))\n",
        "\n",
        "        return loss_value\n",
        "\n",
        "    def evaluate(self, dataset):\n",
        "        return evaluate(self.checkpoint.model, dataset)\n",
        "\n",
        "    def restore(self):\n",
        "        if self.checkpoint_manager.latest_checkpoint:\n",
        "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
        "            print(f'Model restored from checkpoint at step {self.checkpoint.step.numpy()}.')\n",
        "\n",
        "\n",
        "class EdsrTrainer(Trainer):\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 checkpoint_dir,\n",
        "                 learning_rate=PiecewiseConstantDecay(boundaries=[200000], values=[1e-4, 5e-5])):\n",
        "        super().__init__(model, loss=MeanAbsoluteError(), learning_rate=learning_rate, checkpoint_dir=checkpoint_dir)\n",
        "\n",
        "    def train(self, train_dataset, valid_dataset, steps=300000, evaluate_every=1000, save_best_only=True):\n",
        "        super().train(train_dataset, valid_dataset, steps, evaluate_every, save_best_only)\n",
        "\n",
        "\n",
        "class WdsrTrainer(Trainer):\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 checkpoint_dir,\n",
        "                 learning_rate=PiecewiseConstantDecay(boundaries=[200000], values=[1e-3, 5e-4])):\n",
        "        super().__init__(model, loss=MeanAbsoluteError(), learning_rate=learning_rate, checkpoint_dir=checkpoint_dir)\n",
        "\n",
        "    def train(self, train_dataset, valid_dataset, steps=300000, evaluate_every=1000, save_best_only=True):\n",
        "        super().train(train_dataset, valid_dataset, steps, evaluate_every, save_best_only)\n",
        "\n",
        "\n",
        "class SrganGeneratorTrainer(Trainer):\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 checkpoint_dir,\n",
        "                 learning_rate=1e-4):\n",
        "        super().__init__(model, loss=MeanSquaredError(), learning_rate=learning_rate, checkpoint_dir=checkpoint_dir)\n",
        "\n",
        "    def train(self, train_dataset, valid_dataset, steps=1000000, evaluate_every=1000, save_best_only=True):\n",
        "        super().train(train_dataset, valid_dataset, steps, evaluate_every, save_best_only)\n",
        "\n",
        "\n",
        "class SrganTrainer:\n",
        "\n",
        "    def __init__(self,\n",
        "                 generator,\n",
        "                 discriminator,\n",
        "                 content_loss='VGG54',\n",
        "                 learning_rate=PiecewiseConstantDecay(boundaries=[100000], values=[1e-4, 1e-5])):\n",
        "\n",
        "        if content_loss == 'VGG22':\n",
        "            self.vgg = vgg_22()\n",
        "        elif content_loss == 'VGG54':\n",
        "            self.vgg = vgg_54()\n",
        "        else:\n",
        "            raise ValueError(\"content_loss must be either 'VGG22' or 'VGG54'\")\n",
        "\n",
        "        self.content_loss = content_loss\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.generator_optimizer = Adam(learning_rate=learning_rate)\n",
        "        self.discriminator_optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "        self.binary_cross_entropy = BinaryCrossentropy(from_logits=False)\n",
        "        self.mean_squared_error = MeanSquaredError()\n",
        "\n",
        "    def train(self, train_dataset, steps=100000):\n",
        "        pls_metric = Mean()\n",
        "        dls_metric = Mean()\n",
        "        step = 0\n",
        "\n",
        "        for lr, hr in train_dataset.take(steps):\n",
        "            step += 1\n",
        "\n",
        "            pl, dl = self.train_step(lr, hr)\n",
        "            pls_metric(pl)\n",
        "            dls_metric(dl)\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                print(f'{step}/{steps}, perceptual loss = {pls_metric.result():.4f}, discriminator loss = {dls_metric.result():.4f}')\n",
        "                pls_metric.reset_states()\n",
        "                dls_metric.reset_states()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, lr, hr):\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            lr = tf.cast(lr, tf.float32)\n",
        "            hr = tf.cast(hr, tf.float32)\n",
        "\n",
        "            sr = self.generator(lr, training=True)\n",
        "\n",
        "            hr_output = self.discriminator(hr, training=True)\n",
        "            sr_output = self.discriminator(sr, training=True)\n",
        "\n",
        "            con_loss = self._content_loss(hr, sr)\n",
        "            gen_loss = self._generator_loss(sr_output)\n",
        "            perc_loss = con_loss + 0.001 * gen_loss\n",
        "            disc_loss = self._discriminator_loss(hr_output, sr_output)\n",
        "\n",
        "        gradients_of_generator = gen_tape.gradient(perc_loss, self.generator.trainable_variables)\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
        "        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
        "\n",
        "        return perc_loss, disc_loss\n",
        "\n",
        "    @tf.function\n",
        "    def _content_loss(self, hr, sr):\n",
        "        sr = preprocess_input(sr)\n",
        "        hr = preprocess_input(hr)\n",
        "        sr_features = self.vgg(sr) / 12.75\n",
        "        hr_features = self.vgg(hr) / 12.75\n",
        "        return self.mean_squared_error(hr_features, sr_features)\n",
        "\n",
        "    def _generator_loss(self, sr_out):\n",
        "        return self.binary_cross_entropy(tf.ones_like(sr_out), sr_out)\n",
        "\n",
        "    def _discriminator_loss(self, hr_out, sr_out):\n",
        "        hr_loss = self.binary_cross_entropy(tf.ones_like(hr_out), hr_out)\n",
        "        sr_loss = self.binary_cross_entropy(tf.zeros_like(sr_out), sr_out)\n",
        "        return hr_loss + sr_loss\n"
      ],
      "metadata": {
        "id": "80EzE5nsNZ2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan_generator = generator()\n",
        "gan_generator.load_weights('/content/drive/MyDrive/image_dir/generatorweights.h5')\n",
        "\n",
        "gan_trainer = SrganTrainer(generator=gan_generator, discriminator=discriminator())\n",
        "\n",
        "gan_trainer.train(train_ds, steps=26350)\n",
        "\n",
        "gan_trainer.generator.save_weights('/content/gan_generator_updated3.h5')\n",
        "gan_trainer.discriminator.save_weights('/content/gan_discriminator-updated.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gs8bUGc4NZ6V",
        "outputId": "93c018ca-6239-4665-8791-180e0bc35315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n",
            "50/100000, perceptual loss = 0.1021, discriminator loss = 2.0205\n",
            "100/100000, perceptual loss = 0.0933, discriminator loss = 1.4232\n",
            "150/100000, perceptual loss = 0.0965, discriminator loss = 1.3939\n",
            "200/100000, perceptual loss = 0.0821, discriminator loss = 1.1785\n",
            "250/100000, perceptual loss = 0.0958, discriminator loss = 0.9566\n",
            "300/100000, perceptual loss = 0.0960, discriminator loss = 0.8682\n",
            "350/100000, perceptual loss = 0.0914, discriminator loss = 0.9443\n",
            "400/100000, perceptual loss = 0.0918, discriminator loss = 0.4668\n",
            "450/100000, perceptual loss = 0.0942, discriminator loss = 0.9127\n",
            "500/100000, perceptual loss = 0.0922, discriminator loss = 1.0120\n",
            "550/100000, perceptual loss = 0.0967, discriminator loss = 0.6521\n",
            "600/100000, perceptual loss = 0.0862, discriminator loss = 0.7819\n",
            "650/100000, perceptual loss = 0.0879, discriminator loss = 0.8680\n",
            "700/100000, perceptual loss = 0.0936, discriminator loss = 0.5154\n",
            "750/100000, perceptual loss = 0.0910, discriminator loss = 0.5454\n",
            "800/100000, perceptual loss = 0.0921, discriminator loss = 0.6714\n",
            "850/100000, perceptual loss = 0.0848, discriminator loss = 0.7511\n",
            "900/100000, perceptual loss = 0.0960, discriminator loss = 0.7266\n",
            "950/100000, perceptual loss = 0.0881, discriminator loss = 0.7985\n",
            "1000/100000, perceptual loss = 0.0923, discriminator loss = 0.9310\n",
            "1050/100000, perceptual loss = 0.0899, discriminator loss = 0.5979\n",
            "1100/100000, perceptual loss = 0.0890, discriminator loss = 0.8472\n",
            "1150/100000, perceptual loss = 0.0833, discriminator loss = 1.1046\n",
            "1200/100000, perceptual loss = 0.0956, discriminator loss = 0.7876\n",
            "1250/100000, perceptual loss = 0.0935, discriminator loss = 0.5995\n",
            "1300/100000, perceptual loss = 0.0888, discriminator loss = 0.9719\n",
            "1350/100000, perceptual loss = 0.0868, discriminator loss = 0.6231\n",
            "1400/100000, perceptual loss = 0.0820, discriminator loss = 1.4945\n",
            "1450/100000, perceptual loss = 0.0894, discriminator loss = 0.5707\n",
            "1500/100000, perceptual loss = 0.0891, discriminator loss = 0.8457\n",
            "1550/100000, perceptual loss = 0.0880, discriminator loss = 0.9951\n",
            "1600/100000, perceptual loss = 0.0855, discriminator loss = 0.6116\n",
            "1650/100000, perceptual loss = 0.0883, discriminator loss = 0.4952\n",
            "1700/100000, perceptual loss = 0.0885, discriminator loss = 0.8954\n",
            "1750/100000, perceptual loss = 0.0870, discriminator loss = 0.5434\n",
            "1800/100000, perceptual loss = 0.0850, discriminator loss = 1.3519\n",
            "1850/100000, perceptual loss = 0.0885, discriminator loss = 0.6916\n",
            "1900/100000, perceptual loss = 0.0824, discriminator loss = 0.8140\n",
            "1950/100000, perceptual loss = 0.0912, discriminator loss = 0.8422\n",
            "2000/100000, perceptual loss = 0.0931, discriminator loss = 0.5407\n",
            "2050/100000, perceptual loss = 0.0851, discriminator loss = 0.4936\n",
            "2100/100000, perceptual loss = 0.0820, discriminator loss = 0.6486\n",
            "2150/100000, perceptual loss = 0.0923, discriminator loss = 0.4370\n",
            "2200/100000, perceptual loss = 0.0894, discriminator loss = 0.4441\n",
            "2250/100000, perceptual loss = 0.0860, discriminator loss = 0.4124\n",
            "2300/100000, perceptual loss = 0.0847, discriminator loss = 0.8935\n",
            "2350/100000, perceptual loss = 0.0875, discriminator loss = 0.6734\n",
            "2400/100000, perceptual loss = 0.0878, discriminator loss = 0.4287\n",
            "2450/100000, perceptual loss = 0.0829, discriminator loss = 0.5845\n",
            "2500/100000, perceptual loss = 0.0888, discriminator loss = 0.6627\n",
            "2550/100000, perceptual loss = 0.0826, discriminator loss = 0.8255\n",
            "2600/100000, perceptual loss = 0.0922, discriminator loss = 0.1912\n",
            "2650/100000, perceptual loss = 0.0835, discriminator loss = 0.7161\n",
            "2700/100000, perceptual loss = 0.0907, discriminator loss = 0.5632\n",
            "2750/100000, perceptual loss = 0.0855, discriminator loss = 0.9625\n",
            "2800/100000, perceptual loss = 0.0822, discriminator loss = 0.7793\n",
            "2850/100000, perceptual loss = 0.0806, discriminator loss = 0.6592\n",
            "2900/100000, perceptual loss = 0.0924, discriminator loss = 0.5430\n",
            "2950/100000, perceptual loss = 0.0857, discriminator loss = 0.9342\n",
            "3000/100000, perceptual loss = 0.0892, discriminator loss = 0.8719\n",
            "3050/100000, perceptual loss = 0.0810, discriminator loss = 0.6645\n",
            "3100/100000, perceptual loss = 0.0870, discriminator loss = 0.8259\n",
            "3150/100000, perceptual loss = 0.0935, discriminator loss = 0.3536\n",
            "3200/100000, perceptual loss = 0.0882, discriminator loss = 1.0776\n",
            "3250/100000, perceptual loss = 0.0851, discriminator loss = 0.6433\n",
            "3300/100000, perceptual loss = 0.0857, discriminator loss = 0.5252\n",
            "3350/100000, perceptual loss = 0.0889, discriminator loss = 0.5576\n",
            "3400/100000, perceptual loss = 0.0825, discriminator loss = 0.5738\n",
            "3450/100000, perceptual loss = 0.0927, discriminator loss = 0.6463\n",
            "3500/100000, perceptual loss = 0.0859, discriminator loss = 0.6871\n",
            "3550/100000, perceptual loss = 0.0841, discriminator loss = 0.7532\n",
            "3600/100000, perceptual loss = 0.0817, discriminator loss = 0.5805\n",
            "3650/100000, perceptual loss = 0.0908, discriminator loss = 0.5586\n",
            "3700/100000, perceptual loss = 0.0879, discriminator loss = 0.7814\n",
            "3750/100000, perceptual loss = 0.0868, discriminator loss = 0.7859\n",
            "3800/100000, perceptual loss = 0.0842, discriminator loss = 0.6280\n",
            "3850/100000, perceptual loss = 0.0889, discriminator loss = 0.6948\n",
            "3900/100000, perceptual loss = 0.0901, discriminator loss = 0.8444\n",
            "3950/100000, perceptual loss = 0.0876, discriminator loss = 0.4595\n",
            "4000/100000, perceptual loss = 0.0895, discriminator loss = 0.3102\n",
            "4050/100000, perceptual loss = 0.0861, discriminator loss = 0.2917\n",
            "4100/100000, perceptual loss = 0.0932, discriminator loss = 0.7021\n",
            "4150/100000, perceptual loss = 0.0825, discriminator loss = 0.6454\n",
            "4200/100000, perceptual loss = 0.0857, discriminator loss = 0.3789\n",
            "4250/100000, perceptual loss = 0.0824, discriminator loss = 0.4072\n",
            "4300/100000, perceptual loss = 0.0850, discriminator loss = 1.0471\n",
            "4350/100000, perceptual loss = 0.0835, discriminator loss = 0.5610\n",
            "4400/100000, perceptual loss = 0.0872, discriminator loss = 0.5563\n",
            "4450/100000, perceptual loss = 0.0875, discriminator loss = 0.6084\n",
            "4500/100000, perceptual loss = 0.0823, discriminator loss = 0.3431\n",
            "4550/100000, perceptual loss = 0.0803, discriminator loss = 0.6274\n",
            "4600/100000, perceptual loss = 0.0904, discriminator loss = 0.5061\n",
            "4650/100000, perceptual loss = 0.0856, discriminator loss = 0.7515\n",
            "4700/100000, perceptual loss = 0.0848, discriminator loss = 0.5831\n",
            "4750/100000, perceptual loss = 0.0824, discriminator loss = 0.6424\n",
            "4800/100000, perceptual loss = 0.0866, discriminator loss = 0.5008\n",
            "4850/100000, perceptual loss = 0.0904, discriminator loss = 0.6306\n",
            "4900/100000, perceptual loss = 0.0837, discriminator loss = 0.5139\n",
            "4950/100000, perceptual loss = 0.0851, discriminator loss = 0.5217\n",
            "5000/100000, perceptual loss = 0.0862, discriminator loss = 0.4188\n",
            "5050/100000, perceptual loss = 0.0924, discriminator loss = 0.4616\n",
            "5100/100000, perceptual loss = 0.0840, discriminator loss = 0.6082\n",
            "5150/100000, perceptual loss = 0.0832, discriminator loss = 0.5381\n",
            "5200/100000, perceptual loss = 0.0857, discriminator loss = 0.5114\n",
            "5250/100000, perceptual loss = 0.0840, discriminator loss = 0.5468\n",
            "5300/100000, perceptual loss = 0.0831, discriminator loss = 1.0139\n",
            "5350/100000, perceptual loss = 0.0932, discriminator loss = 0.3839\n",
            "5400/100000, perceptual loss = 0.0843, discriminator loss = 0.5799\n",
            "5450/100000, perceptual loss = 0.0861, discriminator loss = 0.5508\n",
            "5500/100000, perceptual loss = 0.0795, discriminator loss = 0.5103\n",
            "5550/100000, perceptual loss = 0.0899, discriminator loss = 0.8105\n",
            "5600/100000, perceptual loss = 0.0890, discriminator loss = 0.9826\n",
            "5650/100000, perceptual loss = 0.0831, discriminator loss = 0.6167\n",
            "5700/100000, perceptual loss = 0.0838, discriminator loss = 0.4353\n",
            "5750/100000, perceptual loss = 0.0843, discriminator loss = 0.7903\n",
            "5800/100000, perceptual loss = 0.0901, discriminator loss = 0.5041\n",
            "5850/100000, perceptual loss = 0.0856, discriminator loss = 1.0162\n",
            "5900/100000, perceptual loss = 0.0872, discriminator loss = 0.2435\n",
            "5950/100000, perceptual loss = 0.0889, discriminator loss = 0.3379\n",
            "6000/100000, perceptual loss = 0.0882, discriminator loss = 0.7033\n",
            "6050/100000, perceptual loss = 0.0783, discriminator loss = 0.6582\n",
            "6100/100000, perceptual loss = 0.0876, discriminator loss = 0.2916\n",
            "6150/100000, perceptual loss = 0.0844, discriminator loss = 0.5711\n",
            "6200/100000, perceptual loss = 0.0835, discriminator loss = 0.4439\n",
            "6250/100000, perceptual loss = 0.0808, discriminator loss = 0.4555\n",
            "6300/100000, perceptual loss = 0.0858, discriminator loss = 0.4871\n",
            "6350/100000, perceptual loss = 0.0932, discriminator loss = 0.7171\n",
            "6400/100000, perceptual loss = 0.0831, discriminator loss = 0.5187\n",
            "6450/100000, perceptual loss = 0.0834, discriminator loss = 0.3732\n",
            "6500/100000, perceptual loss = 0.0844, discriminator loss = 0.6051\n",
            "6550/100000, perceptual loss = 0.0850, discriminator loss = 0.6876\n",
            "6600/100000, perceptual loss = 0.0784, discriminator loss = 0.4719\n",
            "6650/100000, perceptual loss = 0.0864, discriminator loss = 0.6197\n",
            "6700/100000, perceptual loss = 0.0860, discriminator loss = 0.6005\n",
            "6750/100000, perceptual loss = 0.0893, discriminator loss = 0.4805\n",
            "6800/100000, perceptual loss = 0.0781, discriminator loss = 0.5994\n",
            "6850/100000, perceptual loss = 0.0911, discriminator loss = 0.6754\n",
            "6900/100000, perceptual loss = 0.0871, discriminator loss = 0.4913\n",
            "6950/100000, perceptual loss = 0.0893, discriminator loss = 0.4483\n",
            "7000/100000, perceptual loss = 0.0823, discriminator loss = 0.5907\n",
            "7050/100000, perceptual loss = 0.0932, discriminator loss = 0.9530\n",
            "7100/100000, perceptual loss = 0.0853, discriminator loss = 0.6010\n",
            "7150/100000, perceptual loss = 0.0851, discriminator loss = 0.4596\n",
            "7200/100000, perceptual loss = 0.0823, discriminator loss = 0.4632\n",
            "7250/100000, perceptual loss = 0.0895, discriminator loss = 0.5611\n",
            "7300/100000, perceptual loss = 0.0888, discriminator loss = 0.5872\n",
            "7350/100000, perceptual loss = 0.0863, discriminator loss = 0.6884\n",
            "7400/100000, perceptual loss = 0.0783, discriminator loss = 0.5628\n",
            "7450/100000, perceptual loss = 0.0889, discriminator loss = 0.3943\n",
            "7500/100000, perceptual loss = 0.0865, discriminator loss = 0.4151\n",
            "7550/100000, perceptual loss = 0.0877, discriminator loss = 0.2544\n",
            "7600/100000, perceptual loss = 0.0838, discriminator loss = 0.5512\n",
            "7650/100000, perceptual loss = 0.0844, discriminator loss = 0.5966\n",
            "7700/100000, perceptual loss = 0.0886, discriminator loss = 0.5435\n",
            "7750/100000, perceptual loss = 0.0801, discriminator loss = 0.6436\n",
            "7800/100000, perceptual loss = 0.0860, discriminator loss = 0.3200\n",
            "7850/100000, perceptual loss = 0.0865, discriminator loss = 0.9059\n",
            "7900/100000, perceptual loss = 0.0856, discriminator loss = 0.4794\n",
            "7950/100000, perceptual loss = 0.0776, discriminator loss = 0.4377\n",
            "8000/100000, perceptual loss = 0.0929, discriminator loss = 0.6299\n",
            "8050/100000, perceptual loss = 0.0877, discriminator loss = 0.4510\n",
            "8100/100000, perceptual loss = 0.0833, discriminator loss = 0.3764\n",
            "8150/100000, perceptual loss = 0.0794, discriminator loss = 0.3260\n",
            "8200/100000, perceptual loss = 0.0865, discriminator loss = 0.4577\n",
            "8250/100000, perceptual loss = 0.0871, discriminator loss = 0.5766\n",
            "8300/100000, perceptual loss = 0.0817, discriminator loss = 0.7831\n",
            "8350/100000, perceptual loss = 0.0869, discriminator loss = 0.2193\n",
            "8400/100000, perceptual loss = 0.0852, discriminator loss = 0.3726\n",
            "8450/100000, perceptual loss = 0.0898, discriminator loss = 0.4714\n",
            "8500/100000, perceptual loss = 0.0796, discriminator loss = 0.5714\n",
            "8550/100000, perceptual loss = 0.0895, discriminator loss = 0.3599\n",
            "8600/100000, perceptual loss = 0.0861, discriminator loss = 0.6083\n",
            "8650/100000, perceptual loss = 0.0941, discriminator loss = 0.4632\n",
            "8700/100000, perceptual loss = 0.0779, discriminator loss = 0.5535\n",
            "8750/100000, perceptual loss = 0.0901, discriminator loss = 0.6195\n",
            "8800/100000, perceptual loss = 0.0865, discriminator loss = 0.3937\n",
            "8850/100000, perceptual loss = 0.0835, discriminator loss = 0.5590\n",
            "8900/100000, perceptual loss = 0.0827, discriminator loss = 0.7682\n",
            "8950/100000, perceptual loss = 0.0858, discriminator loss = 0.5230\n",
            "9000/100000, perceptual loss = 0.0852, discriminator loss = 0.4307\n",
            "9050/100000, perceptual loss = 0.0829, discriminator loss = 0.4499\n",
            "9100/100000, perceptual loss = 0.0842, discriminator loss = 0.5169\n",
            "9150/100000, perceptual loss = 0.0855, discriminator loss = 0.5415\n",
            "9200/100000, perceptual loss = 0.0878, discriminator loss = 0.5062\n",
            "9250/100000, perceptual loss = 0.0810, discriminator loss = 0.4071\n",
            "9300/100000, perceptual loss = 0.0849, discriminator loss = 0.3476\n",
            "9350/100000, perceptual loss = 0.0830, discriminator loss = 0.5717\n",
            "9400/100000, perceptual loss = 0.0878, discriminator loss = 0.4907\n",
            "9450/100000, perceptual loss = 0.0783, discriminator loss = 0.3745\n",
            "9500/100000, perceptual loss = 0.0917, discriminator loss = 0.4722\n",
            "9550/100000, perceptual loss = 0.0876, discriminator loss = 0.4816\n",
            "9600/100000, perceptual loss = 0.0830, discriminator loss = 0.6083\n",
            "9650/100000, perceptual loss = 0.0801, discriminator loss = 0.5888\n",
            "9700/100000, perceptual loss = 0.0879, discriminator loss = 0.2060\n",
            "9750/100000, perceptual loss = 0.0852, discriminator loss = 0.7341\n",
            "9800/100000, perceptual loss = 0.0846, discriminator loss = 0.4249\n",
            "9850/100000, perceptual loss = 0.0826, discriminator loss = 0.3449\n",
            "9900/100000, perceptual loss = 0.0864, discriminator loss = 0.4443\n",
            "9950/100000, perceptual loss = 0.0846, discriminator loss = 0.6493\n",
            "10000/100000, perceptual loss = 0.0852, discriminator loss = 0.4994\n",
            "10050/100000, perceptual loss = 0.0864, discriminator loss = 0.4632\n",
            "10100/100000, perceptual loss = 0.0858, discriminator loss = 0.5040\n",
            "10150/100000, perceptual loss = 0.0849, discriminator loss = 0.2421\n",
            "10200/100000, perceptual loss = 0.0792, discriminator loss = 0.7990\n",
            "10250/100000, perceptual loss = 0.0813, discriminator loss = 0.3300\n",
            "10300/100000, perceptual loss = 0.0868, discriminator loss = 0.5684\n",
            "10350/100000, perceptual loss = 0.0914, discriminator loss = 0.5361\n",
            "10400/100000, perceptual loss = 0.0813, discriminator loss = 0.5261\n",
            "10450/100000, perceptual loss = 0.0961, discriminator loss = 0.4436\n",
            "10500/100000, perceptual loss = 0.0896, discriminator loss = 0.3368\n",
            "10550/100000, perceptual loss = 0.0828, discriminator loss = 0.4916\n",
            "10600/100000, perceptual loss = 0.0832, discriminator loss = 0.3521\n",
            "10650/100000, perceptual loss = 0.0916, discriminator loss = 0.3327\n",
            "10700/100000, perceptual loss = 0.0907, discriminator loss = 0.5087\n",
            "10750/100000, perceptual loss = 0.0797, discriminator loss = 0.4819\n",
            "10800/100000, perceptual loss = 0.0813, discriminator loss = 0.5396\n",
            "10850/100000, perceptual loss = 0.0851, discriminator loss = 0.4577\n",
            "10900/100000, perceptual loss = 0.0917, discriminator loss = 0.3757\n",
            "10950/100000, perceptual loss = 0.0860, discriminator loss = 0.3694\n",
            "11000/100000, perceptual loss = 0.0873, discriminator loss = 0.2786\n",
            "11050/100000, perceptual loss = 0.0858, discriminator loss = 0.2894\n",
            "11100/100000, perceptual loss = 0.0904, discriminator loss = 0.1975\n",
            "11150/100000, perceptual loss = 0.0773, discriminator loss = 0.3662\n",
            "11200/100000, perceptual loss = 0.0897, discriminator loss = 0.2387\n",
            "11250/100000, perceptual loss = 0.0840, discriminator loss = 0.4977\n",
            "11300/100000, perceptual loss = 0.0875, discriminator loss = 0.6263\n",
            "11350/100000, perceptual loss = 0.0766, discriminator loss = 0.4320\n",
            "11400/100000, perceptual loss = 0.0871, discriminator loss = 0.5375\n",
            "11450/100000, perceptual loss = 0.0882, discriminator loss = 0.4129\n",
            "11500/100000, perceptual loss = 0.0855, discriminator loss = 0.2947\n",
            "11550/100000, perceptual loss = 0.0857, discriminator loss = 0.3759\n",
            "11600/100000, perceptual loss = 0.0882, discriminator loss = 0.3418\n",
            "11650/100000, perceptual loss = 0.0900, discriminator loss = 0.6309\n",
            "11700/100000, perceptual loss = 0.0876, discriminator loss = 0.3678\n",
            "11750/100000, perceptual loss = 0.0859, discriminator loss = 0.4915\n",
            "11800/100000, perceptual loss = 0.0823, discriminator loss = 0.3965\n",
            "11850/100000, perceptual loss = 0.0856, discriminator loss = 0.6907\n",
            "11900/100000, perceptual loss = 0.0842, discriminator loss = 0.4329\n",
            "11950/100000, perceptual loss = 0.0889, discriminator loss = 0.4239\n",
            "12000/100000, perceptual loss = 0.0833, discriminator loss = 0.6513\n",
            "12050/100000, perceptual loss = 0.0870, discriminator loss = 0.2727\n",
            "12100/100000, perceptual loss = 0.0778, discriminator loss = 0.5652\n",
            "12150/100000, perceptual loss = 0.0897, discriminator loss = 0.5489\n",
            "12200/100000, perceptual loss = 0.0808, discriminator loss = 0.4525\n",
            "12250/100000, perceptual loss = 0.0823, discriminator loss = 0.3557\n",
            "12300/100000, perceptual loss = 0.0806, discriminator loss = 0.3057\n",
            "12350/100000, perceptual loss = 0.0899, discriminator loss = 0.3946\n",
            "12400/100000, perceptual loss = 0.0862, discriminator loss = 0.2993\n",
            "12450/100000, perceptual loss = 0.0850, discriminator loss = 0.6559\n",
            "12500/100000, perceptual loss = 0.0827, discriminator loss = 0.3115\n",
            "12550/100000, perceptual loss = 0.0777, discriminator loss = 0.3774\n",
            "12600/100000, perceptual loss = 0.0869, discriminator loss = 0.2640\n",
            "12650/100000, perceptual loss = 0.0802, discriminator loss = 0.4400\n",
            "12700/100000, perceptual loss = 0.0833, discriminator loss = 0.3919\n",
            "12750/100000, perceptual loss = 0.0888, discriminator loss = 0.3295\n",
            "12800/100000, perceptual loss = 0.0886, discriminator loss = 0.4200\n",
            "12850/100000, perceptual loss = 0.0817, discriminator loss = 0.2336\n",
            "12900/100000, perceptual loss = 0.0883, discriminator loss = 0.6440\n",
            "12950/100000, perceptual loss = 0.0853, discriminator loss = 0.4906\n",
            "13000/100000, perceptual loss = 0.0851, discriminator loss = 0.3722\n",
            "13050/100000, perceptual loss = 0.0789, discriminator loss = 0.2378\n",
            "13100/100000, perceptual loss = 0.0899, discriminator loss = 0.6440\n",
            "13150/100000, perceptual loss = 0.0881, discriminator loss = 0.3542\n",
            "13200/100000, perceptual loss = 0.0864, discriminator loss = 0.4876\n",
            "13250/100000, perceptual loss = 0.0805, discriminator loss = 0.1778\n",
            "13300/100000, perceptual loss = 0.0847, discriminator loss = 0.3588\n",
            "13350/100000, perceptual loss = 0.0884, discriminator loss = 0.2341\n",
            "13400/100000, perceptual loss = 0.0850, discriminator loss = 0.7044\n",
            "13450/100000, perceptual loss = 0.0811, discriminator loss = 0.3292\n",
            "13500/100000, perceptual loss = 0.0813, discriminator loss = 0.3938\n",
            "13550/100000, perceptual loss = 0.0880, discriminator loss = 0.3667\n",
            "13600/100000, perceptual loss = 0.0782, discriminator loss = 0.3277\n",
            "13650/100000, perceptual loss = 0.0860, discriminator loss = 0.4596\n",
            "13700/100000, perceptual loss = 0.0858, discriminator loss = 0.3040\n",
            "13750/100000, perceptual loss = 0.0818, discriminator loss = 0.3581\n",
            "13800/100000, perceptual loss = 0.0793, discriminator loss = 0.2307\n",
            "13850/100000, perceptual loss = 0.0857, discriminator loss = 0.4033\n",
            "13900/100000, perceptual loss = 0.0866, discriminator loss = 0.4242\n",
            "13950/100000, perceptual loss = 0.0850, discriminator loss = 0.6573\n",
            "14000/100000, perceptual loss = 0.0825, discriminator loss = 0.3696\n",
            "14050/100000, perceptual loss = 0.0836, discriminator loss = 0.4460\n",
            "14100/100000, perceptual loss = 0.0866, discriminator loss = 0.5174\n",
            "14150/100000, perceptual loss = 0.0825, discriminator loss = 0.2230\n",
            "14200/100000, perceptual loss = 0.0850, discriminator loss = 0.5203\n",
            "14250/100000, perceptual loss = 0.0837, discriminator loss = 0.2868\n",
            "14300/100000, perceptual loss = 0.0882, discriminator loss = 0.2342\n",
            "14350/100000, perceptual loss = 0.0826, discriminator loss = 0.3202\n",
            "14400/100000, perceptual loss = 0.0847, discriminator loss = 0.2487\n",
            "14450/100000, perceptual loss = 0.0846, discriminator loss = 0.4814\n",
            "14500/100000, perceptual loss = 0.0860, discriminator loss = 0.5832\n",
            "14550/100000, perceptual loss = 0.0762, discriminator loss = 0.4123\n",
            "14600/100000, perceptual loss = 0.0874, discriminator loss = 0.4047\n",
            "14650/100000, perceptual loss = 0.0866, discriminator loss = 0.2508\n",
            "14700/100000, perceptual loss = 0.0829, discriminator loss = 0.3520\n",
            "14750/100000, perceptual loss = 0.0762, discriminator loss = 0.3798\n",
            "14800/100000, perceptual loss = 0.0843, discriminator loss = 0.6648\n",
            "14850/100000, perceptual loss = 0.0833, discriminator loss = 0.2056\n",
            "14900/100000, perceptual loss = 0.0841, discriminator loss = 0.3252\n",
            "14950/100000, perceptual loss = 0.0790, discriminator loss = 0.3662\n",
            "15000/100000, perceptual loss = 0.0833, discriminator loss = 0.3092\n",
            "15050/100000, perceptual loss = 0.0892, discriminator loss = 0.2081\n",
            "15100/100000, perceptual loss = 0.0860, discriminator loss = 0.5955\n",
            "15150/100000, perceptual loss = 0.0827, discriminator loss = 0.3625\n",
            "15200/100000, perceptual loss = 0.0796, discriminator loss = 0.2884\n",
            "15250/100000, perceptual loss = 0.0863, discriminator loss = 0.4273\n",
            "15300/100000, perceptual loss = 0.0816, discriminator loss = 0.2495\n",
            "15350/100000, perceptual loss = 0.0897, discriminator loss = 0.2726\n",
            "15400/100000, perceptual loss = 0.0845, discriminator loss = 0.2956\n",
            "15450/100000, perceptual loss = 0.0834, discriminator loss = 0.6027\n",
            "15500/100000, perceptual loss = 0.0766, discriminator loss = 0.4167\n",
            "15550/100000, perceptual loss = 0.0873, discriminator loss = 0.2482\n",
            "15600/100000, perceptual loss = 0.0852, discriminator loss = 0.6424\n",
            "15650/100000, perceptual loss = 0.0806, discriminator loss = 0.3519\n",
            "15700/100000, perceptual loss = 0.0784, discriminator loss = 0.4681\n",
            "15750/100000, perceptual loss = 0.0847, discriminator loss = 0.3916\n",
            "15800/100000, perceptual loss = 0.0896, discriminator loss = 0.3450\n",
            "15850/100000, perceptual loss = 0.0831, discriminator loss = 0.6210\n",
            "15900/100000, perceptual loss = 0.0836, discriminator loss = 0.3705\n",
            "15950/100000, perceptual loss = 0.0818, discriminator loss = 0.4628\n",
            "16000/100000, perceptual loss = 0.0886, discriminator loss = 0.6994\n",
            "16050/100000, perceptual loss = 0.0802, discriminator loss = 0.3220\n",
            "16100/100000, perceptual loss = 0.0836, discriminator loss = 0.6246\n",
            "16150/100000, perceptual loss = 0.0824, discriminator loss = 0.4396\n",
            "16200/100000, perceptual loss = 0.0834, discriminator loss = 0.4760\n",
            "16250/100000, perceptual loss = 0.0793, discriminator loss = 0.2325\n",
            "16300/100000, perceptual loss = 0.0895, discriminator loss = 0.3457\n",
            "16350/100000, perceptual loss = 0.0846, discriminator loss = 0.4340\n",
            "16400/100000, perceptual loss = 0.0832, discriminator loss = 0.4711\n",
            "16450/100000, perceptual loss = 0.0743, discriminator loss = 0.2387\n",
            "16500/100000, perceptual loss = 0.0858, discriminator loss = 0.3580\n",
            "16550/100000, perceptual loss = 0.0853, discriminator loss = 0.2982\n",
            "16600/100000, perceptual loss = 0.0833, discriminator loss = 0.2521\n",
            "16650/100000, perceptual loss = 0.0858, discriminator loss = 0.6120\n",
            "16700/100000, perceptual loss = 0.0836, discriminator loss = 0.2933\n",
            "16750/100000, perceptual loss = 0.0871, discriminator loss = 0.4109\n",
            "16800/100000, perceptual loss = 0.0817, discriminator loss = 0.3621\n",
            "16850/100000, perceptual loss = 0.0836, discriminator loss = 0.3356\n",
            "16900/100000, perceptual loss = 0.0817, discriminator loss = 0.2156\n",
            "16950/100000, perceptual loss = 0.0892, discriminator loss = 0.1803\n",
            "17000/100000, perceptual loss = 0.0834, discriminator loss = 0.2606\n",
            "17050/100000, perceptual loss = 0.0902, discriminator loss = 0.4065\n",
            "17100/100000, perceptual loss = 0.0899, discriminator loss = 0.3943\n",
            "17150/100000, perceptual loss = 0.0855, discriminator loss = 0.2465\n",
            "17200/100000, perceptual loss = 0.0765, discriminator loss = 0.3423\n",
            "17250/100000, perceptual loss = 0.0890, discriminator loss = 0.2318\n",
            "17300/100000, perceptual loss = 0.0848, discriminator loss = 0.3957\n",
            "17350/100000, perceptual loss = 0.0860, discriminator loss = 0.1839\n",
            "17400/100000, perceptual loss = 0.0859, discriminator loss = 0.3711\n",
            "17450/100000, perceptual loss = 0.0874, discriminator loss = 0.0968\n",
            "17500/100000, perceptual loss = 0.0834, discriminator loss = 0.5243\n",
            "17550/100000, perceptual loss = 0.0796, discriminator loss = 0.2759\n",
            "17600/100000, perceptual loss = 0.0863, discriminator loss = 0.2416\n",
            "17650/100000, perceptual loss = 0.0833, discriminator loss = 0.3802\n",
            "17700/100000, perceptual loss = 0.0870, discriminator loss = 0.2159\n",
            "17750/100000, perceptual loss = 0.0855, discriminator loss = 0.5324\n",
            "17800/100000, perceptual loss = 0.0781, discriminator loss = 0.2545\n",
            "17850/100000, perceptual loss = 0.0830, discriminator loss = 0.3423\n",
            "17900/100000, perceptual loss = 0.0890, discriminator loss = 0.2610\n",
            "17950/100000, perceptual loss = 0.0856, discriminator loss = 0.4821\n",
            "18000/100000, perceptual loss = 0.0870, discriminator loss = 0.2182\n",
            "18050/100000, perceptual loss = 0.0789, discriminator loss = 0.4444\n",
            "18100/100000, perceptual loss = 0.0851, discriminator loss = 0.3088\n",
            "18150/100000, perceptual loss = 0.0792, discriminator loss = 0.2891\n",
            "18200/100000, perceptual loss = 0.0910, discriminator loss = 0.4725\n",
            "18250/100000, perceptual loss = 0.0873, discriminator loss = 0.4388\n",
            "18300/100000, perceptual loss = 0.0846, discriminator loss = 0.2880\n",
            "18350/100000, perceptual loss = 0.0820, discriminator loss = 0.3811\n",
            "18400/100000, perceptual loss = 0.0827, discriminator loss = 0.3446\n",
            "18450/100000, perceptual loss = 0.0886, discriminator loss = 0.2326\n",
            "18500/100000, perceptual loss = 0.0833, discriminator loss = 0.1713\n",
            "18550/100000, perceptual loss = 0.0882, discriminator loss = 0.4164\n",
            "18600/100000, perceptual loss = 0.0802, discriminator loss = 0.3385\n",
            "18650/100000, perceptual loss = 0.0880, discriminator loss = 0.5336\n",
            "18700/100000, perceptual loss = 0.0848, discriminator loss = 0.2369\n",
            "18750/100000, perceptual loss = 0.0862, discriminator loss = 0.4295\n",
            "18800/100000, perceptual loss = 0.0842, discriminator loss = 0.2537\n",
            "18850/100000, perceptual loss = 0.0821, discriminator loss = 0.3142\n",
            "18900/100000, perceptual loss = 0.0795, discriminator loss = 0.3545\n",
            "18950/100000, perceptual loss = 0.0912, discriminator loss = 0.4201\n",
            "19000/100000, perceptual loss = 0.0866, discriminator loss = 0.3449\n",
            "19050/100000, perceptual loss = 0.0870, discriminator loss = 0.5036\n",
            "19100/100000, perceptual loss = 0.0785, discriminator loss = 0.2530\n",
            "19150/100000, perceptual loss = 0.0830, discriminator loss = 0.4094\n",
            "19200/100000, perceptual loss = 0.0900, discriminator loss = 0.2625\n",
            "19250/100000, perceptual loss = 0.0848, discriminator loss = 0.3860\n",
            "19300/100000, perceptual loss = 0.0863, discriminator loss = 0.3126\n",
            "19350/100000, perceptual loss = 0.0850, discriminator loss = 0.1892\n",
            "19400/100000, perceptual loss = 0.0934, discriminator loss = 0.2804\n",
            "19450/100000, perceptual loss = 0.0828, discriminator loss = 0.2839\n",
            "19500/100000, perceptual loss = 0.0869, discriminator loss = 0.5987\n",
            "19550/100000, perceptual loss = 0.0829, discriminator loss = 0.1917\n",
            "19600/100000, perceptual loss = 0.0831, discriminator loss = 0.3539\n",
            "19650/100000, perceptual loss = 0.0792, discriminator loss = 0.2734\n",
            "19700/100000, perceptual loss = 0.0913, discriminator loss = 0.4267\n",
            "19750/100000, perceptual loss = 0.0865, discriminator loss = 0.4376\n",
            "19800/100000, perceptual loss = 0.0841, discriminator loss = 0.2552\n",
            "19850/100000, perceptual loss = 0.0777, discriminator loss = 0.2986\n",
            "19900/100000, perceptual loss = 0.0880, discriminator loss = 0.2039\n",
            "19950/100000, perceptual loss = 0.0839, discriminator loss = 0.5664\n",
            "20000/100000, perceptual loss = 0.0868, discriminator loss = 0.2734\n",
            "20050/100000, perceptual loss = 0.0795, discriminator loss = 0.4638\n",
            "20100/100000, perceptual loss = 0.0866, discriminator loss = 0.3296\n",
            "20150/100000, perceptual loss = 0.0887, discriminator loss = 0.4175\n",
            "20200/100000, perceptual loss = 0.0825, discriminator loss = 0.2555\n",
            "20250/100000, perceptual loss = 0.0851, discriminator loss = 0.3655\n",
            "20300/100000, perceptual loss = 0.0838, discriminator loss = 0.2469\n",
            "20350/100000, perceptual loss = 0.0875, discriminator loss = 0.4115\n",
            "20400/100000, perceptual loss = 0.0820, discriminator loss = 0.4036\n",
            "20450/100000, perceptual loss = 0.0857, discriminator loss = 0.2269\n",
            "20500/100000, perceptual loss = 0.0813, discriminator loss = 0.3019\n",
            "20550/100000, perceptual loss = 0.0877, discriminator loss = 0.2060\n",
            "20600/100000, perceptual loss = 0.0802, discriminator loss = 0.1613\n",
            "20650/100000, perceptual loss = 0.0945, discriminator loss = 0.1182\n",
            "20700/100000, perceptual loss = 0.0918, discriminator loss = 0.4053\n",
            "20750/100000, perceptual loss = 0.0859, discriminator loss = 0.2500\n",
            "20800/100000, perceptual loss = 0.0809, discriminator loss = 0.4835\n",
            "20850/100000, perceptual loss = 0.0867, discriminator loss = 0.3586\n",
            "20900/100000, perceptual loss = 0.0881, discriminator loss = 0.3876\n",
            "20950/100000, perceptual loss = 0.0900, discriminator loss = 0.0834\n",
            "21000/100000, perceptual loss = 0.0812, discriminator loss = 0.1315\n",
            "21050/100000, perceptual loss = 0.0909, discriminator loss = 0.5460\n",
            "21100/100000, perceptual loss = 0.0869, discriminator loss = 0.2795\n",
            "21150/100000, perceptual loss = 0.0881, discriminator loss = 0.1743\n",
            "21200/100000, perceptual loss = 0.0872, discriminator loss = 0.2599\n",
            "21250/100000, perceptual loss = 0.0840, discriminator loss = 0.0522\n",
            "21300/100000, perceptual loss = 0.0860, discriminator loss = 0.3706\n",
            "21350/100000, perceptual loss = 0.0766, discriminator loss = 0.1195\n",
            "21400/100000, perceptual loss = 0.0919, discriminator loss = 0.1371\n",
            "21450/100000, perceptual loss = 0.0928, discriminator loss = 0.4864\n",
            "21500/100000, perceptual loss = 0.0898, discriminator loss = 0.1339\n",
            "21550/100000, perceptual loss = 0.0799, discriminator loss = 0.0560\n",
            "21600/100000, perceptual loss = 0.0915, discriminator loss = 0.0874\n",
            "21650/100000, perceptual loss = 0.0920, discriminator loss = 0.3196\n",
            "21700/100000, perceptual loss = 0.0820, discriminator loss = 0.3621\n",
            "21750/100000, perceptual loss = 0.0801, discriminator loss = 0.1345\n",
            "21800/100000, perceptual loss = 0.0875, discriminator loss = 0.1501\n",
            "21850/100000, perceptual loss = 0.0893, discriminator loss = 0.0469\n",
            "21900/100000, perceptual loss = 0.0812, discriminator loss = 0.1376\n",
            "21950/100000, perceptual loss = 0.0836, discriminator loss = 0.1283\n",
            "22000/100000, perceptual loss = 0.0857, discriminator loss = 0.2667\n",
            "22050/100000, perceptual loss = 0.0879, discriminator loss = 0.1415\n",
            "22100/100000, perceptual loss = 0.0820, discriminator loss = 0.1190\n",
            "22150/100000, perceptual loss = 0.0863, discriminator loss = 0.0211\n",
            "22200/100000, perceptual loss = 0.0824, discriminator loss = 0.0415\n",
            "22250/100000, perceptual loss = 0.0902, discriminator loss = 0.1280\n",
            "22300/100000, perceptual loss = 0.0794, discriminator loss = 0.1065\n",
            "22350/100000, perceptual loss = 0.0871, discriminator loss = 0.1255\n",
            "22400/100000, perceptual loss = 0.0907, discriminator loss = 0.0936\n",
            "22450/100000, perceptual loss = 0.0922, discriminator loss = 0.2856\n",
            "22500/100000, perceptual loss = 0.0827, discriminator loss = 0.2492\n",
            "22550/100000, perceptual loss = 0.0900, discriminator loss = 0.0766\n",
            "22600/100000, perceptual loss = 0.0814, discriminator loss = 0.2583\n",
            "22650/100000, perceptual loss = 0.0872, discriminator loss = 0.0724\n",
            "22700/100000, perceptual loss = 0.0830, discriminator loss = 0.2163\n",
            "22750/100000, perceptual loss = 0.0889, discriminator loss = 0.0844\n",
            "22800/100000, perceptual loss = 0.0943, discriminator loss = 0.2166\n",
            "22850/100000, perceptual loss = 0.0898, discriminator loss = 0.1113\n",
            "22900/100000, perceptual loss = 0.0900, discriminator loss = 0.1413\n",
            "22950/100000, perceptual loss = 0.0874, discriminator loss = 0.1169\n",
            "23000/100000, perceptual loss = 0.0892, discriminator loss = 0.1177\n",
            "23050/100000, perceptual loss = 0.0850, discriminator loss = 0.0512\n",
            "23100/100000, perceptual loss = 0.0876, discriminator loss = 0.0349\n",
            "23150/100000, perceptual loss = 0.0891, discriminator loss = 0.2128\n",
            "23200/100000, perceptual loss = 0.0873, discriminator loss = 0.1504\n",
            "23250/100000, perceptual loss = 0.0826, discriminator loss = 0.1730\n",
            "23300/100000, perceptual loss = 0.0882, discriminator loss = 0.1079\n",
            "23350/100000, perceptual loss = 0.0921, discriminator loss = 0.1694\n",
            "23400/100000, perceptual loss = 0.0888, discriminator loss = 0.0580\n",
            "23450/100000, perceptual loss = 0.0835, discriminator loss = 0.0938\n",
            "23500/100000, perceptual loss = 0.0892, discriminator loss = 0.0136\n",
            "23550/100000, perceptual loss = 0.0904, discriminator loss = 0.0776\n",
            "23600/100000, perceptual loss = 0.0904, discriminator loss = 0.0567\n",
            "23650/100000, perceptual loss = 0.0863, discriminator loss = 0.0522\n",
            "23700/100000, perceptual loss = 0.0917, discriminator loss = 0.1821\n",
            "23750/100000, perceptual loss = 0.0926, discriminator loss = 0.0923\n",
            "23800/100000, perceptual loss = 0.0842, discriminator loss = 0.0926\n",
            "23850/100000, perceptual loss = 0.0899, discriminator loss = 0.0339\n",
            "23900/100000, perceptual loss = 0.0898, discriminator loss = 0.0475\n",
            "23950/100000, perceptual loss = 0.0949, discriminator loss = 0.2047\n",
            "24000/100000, perceptual loss = 0.0844, discriminator loss = 0.1350\n",
            "24050/100000, perceptual loss = 0.0964, discriminator loss = 0.1581\n",
            "24100/100000, perceptual loss = 0.0915, discriminator loss = 0.1815\n",
            "24150/100000, perceptual loss = 0.0939, discriminator loss = 0.1086\n",
            "24200/100000, perceptual loss = 0.0827, discriminator loss = 0.1425\n",
            "24250/100000, perceptual loss = 0.0885, discriminator loss = 0.0676\n",
            "24300/100000, perceptual loss = 0.0921, discriminator loss = 0.0666\n",
            "24350/100000, perceptual loss = 0.0855, discriminator loss = 0.0744\n",
            "24400/100000, perceptual loss = 0.0862, discriminator loss = 0.1819\n",
            "24450/100000, perceptual loss = 0.0943, discriminator loss = 0.0535\n",
            "24500/100000, perceptual loss = 0.0854, discriminator loss = 0.0848\n",
            "24550/100000, perceptual loss = 0.0875, discriminator loss = 0.0421\n",
            "24600/100000, perceptual loss = 0.0882, discriminator loss = 0.0345\n",
            "24650/100000, perceptual loss = 0.0890, discriminator loss = 0.0374\n",
            "24700/100000, perceptual loss = 0.0937, discriminator loss = 0.1990\n",
            "24750/100000, perceptual loss = 0.0856, discriminator loss = 0.3945\n",
            "24800/100000, perceptual loss = 0.0859, discriminator loss = 0.0405\n",
            "24850/100000, perceptual loss = 0.0860, discriminator loss = 0.1473\n",
            "24900/100000, perceptual loss = 0.0867, discriminator loss = 0.0628\n",
            "24950/100000, perceptual loss = 0.0831, discriminator loss = 0.0248\n",
            "25000/100000, perceptual loss = 0.0960, discriminator loss = 0.0509\n",
            "25050/100000, perceptual loss = 0.0863, discriminator loss = 0.0392\n",
            "25100/100000, perceptual loss = 0.0849, discriminator loss = 0.0637\n",
            "25150/100000, perceptual loss = 0.0890, discriminator loss = 0.0209\n",
            "25200/100000, perceptual loss = 0.0938, discriminator loss = 0.0193\n",
            "25250/100000, perceptual loss = 0.0925, discriminator loss = 0.0352\n",
            "25300/100000, perceptual loss = 0.0883, discriminator loss = 0.1124\n",
            "25350/100000, perceptual loss = 0.0882, discriminator loss = 0.1647\n",
            "25400/100000, perceptual loss = 0.0891, discriminator loss = 0.1441\n",
            "25450/100000, perceptual loss = 0.0911, discriminator loss = 0.0977\n",
            "25500/100000, perceptual loss = 0.0907, discriminator loss = 0.1117\n",
            "25550/100000, perceptual loss = 0.0890, discriminator loss = 0.1774\n",
            "25600/100000, perceptual loss = 0.0926, discriminator loss = 0.1620\n",
            "25650/100000, perceptual loss = 0.0871, discriminator loss = 0.3806\n",
            "25700/100000, perceptual loss = 0.0764, discriminator loss = 0.2658\n",
            "25750/100000, perceptual loss = 0.0896, discriminator loss = 0.0737\n",
            "25800/100000, perceptual loss = 0.0893, discriminator loss = 0.0353\n",
            "25850/100000, perceptual loss = 0.0877, discriminator loss = 0.0807\n",
            "25900/100000, perceptual loss = 0.0798, discriminator loss = 0.0435\n",
            "25950/100000, perceptual loss = 0.0884, discriminator loss = 0.0526\n",
            "26000/100000, perceptual loss = 0.0860, discriminator loss = 0.0942\n",
            "26050/100000, perceptual loss = 0.0902, discriminator loss = 0.1904\n",
            "26100/100000, perceptual loss = 0.0854, discriminator loss = 0.0933\n",
            "26150/100000, perceptual loss = 0.0949, discriminator loss = 0.1810\n",
            "26200/100000, perceptual loss = 0.0898, discriminator loss = 0.0691\n",
            "26250/100000, perceptual loss = 0.0894, discriminator loss = 0.1234\n",
            "26300/100000, perceptual loss = 0.0916, discriminator loss = 0.0857\n",
            "26350/100000, perceptual loss = 0.0914, discriminator loss = 0.0266\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-662ed5061c45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Train the GAN with 200,000 steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgan_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Save weights of generator and discriminator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-bcb446ca4d95>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, steps)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mpls_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mdls_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gan_trainer.generator.save_weights('/content/gan_generator_updated3.h5')\n",
        "gan_trainer.discriminator.save_weights('/content/gan_discriminator-updated.h5')"
      ],
      "metadata": {
        "id": "QGXkbat-6FmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}